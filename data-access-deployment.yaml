# data-access-deployment.yaml
apiVersion: v1
kind: ServiceAccount # KSA for Workload Identity
metadata:
  name: data-access-ksa # Name used in IAM binding
  namespace: job-app
  annotations: # Link KSA to GSA
    iam.gke.io/gcp-service-account: data-access-sa@projectcloud-451415.iam.gserviceaccount.com
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-access-deployment
  namespace: job-app
  labels:
    app: data-access
spec:
  replicas: 1 # Configure rolling update strategy below
  selector:
    matchLabels:
      app: data-access
  strategy: # Define Rolling Update strategy
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25% # Or 1
      maxSurge: 25% # Or 1
  template:
    metadata:
      labels:
        app: data-access
    spec:
      serviceAccountName: data-access-ksa # Use the KSA configured for Workload Identity
      # Remove the volumes and volumeMounts sections related to bigquery-credentials
      # Remove the GOOGLE_APPLICATION_CREDENTIALS environment variable

      containers:
      # Application Container
      - name: data-access
        image: europe-west1-docker.pkg.dev/projectcloud-451415/cloudapiservice/data_access:latest # Your actual image
        imagePullPolicy: Always
        ports:
        - containerPort: 50051
          name: grpc-access
        env:
          # --- REMOVE BigQuery env vars ---
          # - name: GCP_PROJECT_ID
          #   value: "genial-analyzer-451415-r2"
          # - name: BIGQUERY_DATASET
          #   value: "dataset_jobs"
          # - name: GOOGLE_APPLICATION_CREDENTIALS
          #   value: /etc/secrets/key.json
          # --- END REMOVE ---

          # --- ADD/UPDATE PostgreSQL env vars ---
          # DB_HOST should point to the internal Kubernetes Service name
          - name: DB_HOST
            value: "postgres-service" # Use the name of your PostgreSQL service

          # DB_PORT should point to the service port
          - name: DB_PORT
            value: "5432"

          # Inject DB Name, User, Password from the *new* PostgreSQL Secret
          - name: DB_NAME
            valueFrom:
              secretKeyRef:
                name: postgres-credentials # Must match the name of the Secret you created
                key: POSTGRES_DB
          - name: DB_USER
            valueFrom:
              secretKeyRef:
                name: postgres-credentials # Must match the name of the Secret you created
                key: POSTGRES_USER
          - name: DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: postgres-credentials # Must match the name of the Secret you created
                key: POSTGRES_PASSWORD
          # --- END ADD/UPDATE ---

        # Define Resources (Goal: cost-effective) - ADJUST!
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "250m"
        # Define Probes (Goal: setup your own probes) - ADJUST!
        livenessProbe:
          tcpSocket: { port: grpc-access } # Basic check
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          tcpSocket: { port: grpc-access } # Basic check.
          # CONSIDER: Update this to an exec probe that checks the DB connection
          # e.g., exec: { command: ["pg_isready", "-h", "127.0.0.1", "-p", "5432", "-U", "$(DB_USER)"] }
          # You'll need postgres client tools in your data-access image for pg_isready
          initialDelaySeconds: 5
          periodSeconds: 10
        startupProbe:
          tcpSocket: { port: grpc-access }
          failureThreshold: 30
          periodSeconds: 10

      # --- REMOVE Cloud SQL Auth Proxy Sidecar Container ---
      # - name: cloud-sql-proxy
      #   image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:latest
      #   # ... args and other config ...
      # --- END REMOVE ---